{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install feature_engine","metadata":{"execution":{"iopub.status.busy":"2023-01-08T21:01:29.081403Z","iopub.execute_input":"2023-01-08T21:01:29.081776Z","iopub.status.idle":"2023-01-08T21:01:41.965646Z","shell.execute_reply.started":"2023-01-08T21:01:29.081711Z","shell.execute_reply":"2023-01-08T21:01:41.964430Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting feature_engine\n  Downloading feature_engine-1.4.0-py2.py3-none-any.whl (276 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.4/276.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.7/site-packages (from feature_engine) (1.21.6)\nRequirement already satisfied: statsmodels>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from feature_engine) (0.13.2)\nRequirement already satisfied: pandas>=1.0.3 in /opt/conda/lib/python3.7/site-packages (from feature_engine) (1.3.5)\nRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from feature_engine) (1.7.3)\nRequirement already satisfied: scikit-learn>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from feature_engine) (1.0.2)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.0.3->feature_engine) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.0.3->feature_engine) (2022.1)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=1.0.0->feature_engine) (1.0.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=1.0.0->feature_engine) (3.1.0)\nRequirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.7/site-packages (from statsmodels>=0.11.1->feature_engine) (22.0)\nRequirement already satisfied: patsy>=0.5.2 in /opt/conda/lib/python3.7/site-packages (from statsmodels>=0.11.1->feature_engine) (0.5.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from patsy>=0.5.2->statsmodels>=0.11.1->feature_engine) (1.15.0)\nInstalling collected packages: feature_engine\nSuccessfully installed feature_engine-1.4.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nfrom feature_engine.encoding import WoEEncoder\nfrom sklearn.linear_model import HuberRegressor\nfrom sklearn.impute import KNNImputer\nimport csv","metadata":{"execution":{"iopub.status.busy":"2023-01-08T21:01:41.968876Z","iopub.execute_input":"2023-01-08T21:01:41.969970Z","iopub.status.idle":"2023-01-08T21:01:44.435527Z","shell.execute_reply.started":"2023-01-08T21:01:41.969916Z","shell.execute_reply":"2023-01-08T21:01:44.434454Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Seed\nseed = 123\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\nnp.random.seed(seed)\ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2023-01-08T21:01:44.437406Z","iopub.execute_input":"2023-01-08T21:01:44.438430Z","iopub.status.idle":"2023-01-08T21:01:44.448278Z","shell.execute_reply.started":"2023-01-08T21:01:44.438388Z","shell.execute_reply":"2023-01-08T21:01:44.447280Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"INPUT_PATH = '/kaggle/input/tabular-playground-series-aug-2022/'\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-01-08T21:01:44.451613Z","iopub.execute_input":"2023-01-08T21:01:44.452012Z","iopub.status.idle":"2023-01-08T21:01:44.523610Z","shell.execute_reply.started":"2023-01-08T21:01:44.451976Z","shell.execute_reply":"2023-01-08T21:01:44.522467Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class ValidationDataset(Dataset):\n    def __init__(self, X):\n        self.X = X\n        \n    def __getitem__(self, index):\n        return self.X[index]\n    \n    def __len__(self):\n        return self.X.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-01-08T21:01:44.525380Z","iopub.execute_input":"2023-01-08T21:01:44.525767Z","iopub.status.idle":"2023-01-08T21:01:44.535400Z","shell.execute_reply.started":"2023-01-08T21:01:44.525728Z","shell.execute_reply":"2023-01-08T21:01:44.534297Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(os.path.join(INPUT_PATH, 'train.csv'), index_col='id')\ndf_test = pd.read_csv(os.path.join(INPUT_PATH, 'test.csv'), index_col='id')\ntarget, groups = df_train['failure'], df_train['product_code']\ndf_train.drop('failure',axis=1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-01-08T21:01:44.538914Z","iopub.execute_input":"2023-01-08T21:01:44.539306Z","iopub.status.idle":"2023-01-08T21:01:44.796066Z","shell.execute_reply.started":"2023-01-08T21:01:44.539254Z","shell.execute_reply":"2023-01-08T21:01:44.795013Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def Preprocessing(df_train, df_test, target):\n    # Concatenate training and testing data\n    data = pd.concat([df_train, df_test])\n    \n    # Use dictionaries of dictionary to store the most correlated column according to the product code\n    most_correlated = {}\n    # We manually add data for 'measurement_17' (because it is the most important one among other measurement columns)\n    most_correlated['measurement_17'] = {\n        'A': ['measurement_5','measurement_6','measurement_8'],\n        'B': ['measurement_4','measurement_5','measurement_7'],\n        'C': ['measurement_5','measurement_7','measurement_8','measurement_9'],\n        'D': ['measurement_5','measurement_6','measurement_7','measurement_8'],\n        'E': ['measurement_4','measurement_5','measurement_6','measurement_8'],\n        'F': ['measurement_4','measurement_5','measurement_6','measurement_7'],\n        'G': ['measurement_4','measurement_6','measurement_8','measurement_9'],\n        'H': ['measurement_4','measurement_5','measurement_7','measurement_8','measurement_9'],\n        'I': ['measurement_3','measurement_7','measurement_8']\n    }\n    \n    # From measurement_3 to measurement_16, calculate the sum of the largest 3 correlation values\n    m_cols = [f'measurement_{i}' for i in range(18)]\n    corr_values = []\n    for i in range(3, 17):\n        cur_col = m_cols[i]\n        df_correlation = np.abs(data[m_cols].corr()[cur_col]).sort_values(ascending=False)\n        corr_values.append([cur_col, np.sum(df_correlation[1:4])])\n    \n    # Sorting\n    corr_values = np.array(corr_values)\n    corr_values = corr_values[np.argsort(corr_values[:, 1])[::-1]]\n    \n    # For the 10 most correlated measurement columns\n    # Find other 4 columns that are most correlated to it, and store to dict\n    product_codes = data.product_code.unique()\n    for i in range(10):\n        cur_col = corr_values[i][0]\n        cur_correlated = {}\n        for code in product_codes:\n            df_correlation = np.abs(data[data.product_code == code][m_cols].corr()[cur_col]).sort_values(ascending=False)\n            cur_correlated[code] = df_correlation[1:5].index.tolist()\n        most_correlated[cur_col] = cur_correlated\n    \n    # features that need imputation (measurement columns + loading)\n    features = m_cols + ['loading']\n    \n    # For columns that are highly correlated to other columns, impute with linear model (HuberRegressor)\n    # For all other columns, use KNN imputer\n    for code in product_codes:\n        for cur_col in list(most_correlated.keys()):\n            temp = data[data.product_code == code]\n            corr_cols = most_correlated[cur_col][code]\n            temp_train = temp[corr_cols+[cur_col]].dropna(how='any')\n            temp_test = temp[(temp[cur_col].isnull()) & (temp[corr_cols].isnull().sum(axis=1)==0)]\n            \n            linear_model = HuberRegressor(epsilon=1.9, max_iter=400)\n            linear_model.fit(temp_train[corr_cols], temp_train[cur_col])\n            pred = linear_model.predict(temp_test[corr_cols])\n            data.loc[(data.product_code == code)&(data[cur_col].isnull())&(data[corr_cols].isnull().sum(axis=1)==0), cur_col] = pred\n        \n        knn_model = KNNImputer(n_neighbors=3)\n        data.loc[data.product_code == code, features] = knn_model.fit_transform(data.loc[data.product_code == code, features])\n        \n    \n    # DataFrame of preprocessed data, we will have a total of 10 features\n    preprocessed_data = pd.DataFrame()\n    preprocessed_data['m3_missing'] = data['measurement_3'].isnull().astype(np.int32)\n    preprocessed_data['m5_missing'] = data['measurement_5'].isnull().astype(np.int32)\n    preprocessed_data['area'] = data['attribute_2'] * data['attribute_3']\n    preprocessed_data['measurement_avg'] = data[[f'measurement_{i}' for i in range(3, 17)]].mean(axis=1)\n    # Other features that will be used for model\n    useful_cols = ['loading', 'attribute_0', 'measurement_17', 'measurement_0', 'measurement_1', 'measurement_2']\n    preprocessed_data[useful_cols] = data[useful_cols]\n    \n    # Split training and testing data\n    df_train = preprocessed_data[:df_train.shape[0]]\n    df_test = preprocessed_data[df_train.shape[0]:]\n    \n    # Encode 'attribute_0' with WoEEncoder(Weight of Evidence)\n    woe_encoder = WoEEncoder(variables=['attribute_0'])\n    woe_encoder.fit(df_train, target)\n    df_train = woe_encoder.transform(df_train)\n    df_test = woe_encoder.transform(df_test)\n    \n    # Scale data\n    scaler = StandardScaler()\n    np_train = scaler.fit_transform(df_train)\n    np_test = scaler.transform(df_test)\n    \n    return np_train, np_test","metadata":{"execution":{"iopub.status.busy":"2023-01-08T21:01:44.797738Z","iopub.execute_input":"2023-01-08T21:01:44.798113Z","iopub.status.idle":"2023-01-08T21:01:44.820645Z","shell.execute_reply.started":"2023-01-08T21:01:44.798075Z","shell.execute_reply":"2023-01-08T21:01:44.819540Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"X_train, X_test = Preprocessing(df_train, df_test, target)","metadata":{"execution":{"iopub.status.busy":"2023-01-08T21:01:44.822112Z","iopub.execute_input":"2023-01-08T21:01:44.823035Z","iopub.status.idle":"2023-01-08T21:02:04.473722Z","shell.execute_reply.started":"2023-01-08T21:01:44.822997Z","shell.execute_reply":"2023-01-08T21:02:04.472773Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(10, 32) \n        self.layer_2 = nn.Linear(32, 32)\n        self.layer_3 = nn.Linear(32, 16)\n        self.layer_out = nn.Linear(16, 1)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.4)\n        self.batchnorm1 = nn.BatchNorm1d(32)\n        self.batchnorm2 = nn.BatchNorm1d(16)\n        \n        \n    def forward(self, x):\n        x = self.relu(self.layer_1(x))\n        x = self.batchnorm1(x)\n        x = self.dropout(x)\n        x = self.relu(self.layer_2(x))\n        x = self.batchnorm1(x)\n        x = self.dropout(x)\n        x = self.relu(self.layer_3(x))\n        x = self.batchnorm2(x)\n        x = self.layer_out(x)\n        return x;","metadata":{"execution":{"iopub.status.busy":"2023-01-08T21:02:04.475340Z","iopub.execute_input":"2023-01-08T21:02:04.475701Z","iopub.status.idle":"2023-01-08T21:02:04.485899Z","shell.execute_reply.started":"2023-01-08T21:02:04.475666Z","shell.execute_reply":"2023-01-08T21:02:04.483313Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"val_ds = ValidationDataset(torch.FloatTensor(X_test))\nval_dl = DataLoader(val_ds, batch_size=500, num_workers=2, drop_last=False, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-01-08T21:02:04.488888Z","iopub.execute_input":"2023-01-08T21:02:04.489270Z","iopub.status.idle":"2023-01-08T21:02:04.501646Z","shell.execute_reply.started":"2023-01-08T21:02:04.489233Z","shell.execute_reply":"2023-01-08T21:02:04.500741Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model = Model().to(device)\nmodel.load_state_dict(torch.load('/kaggle/input/model-weights/model_weights.pth'))\n\ncount = 26570\nwith open('submission.csv', 'w', newline='') as csv_file:\n    csv_writer = csv.writer(csv_file)\n    csv_writer.writerow([\"id\", \"failure\"])\n    \n    model.eval()\n\n    for feature in val_dl:\n        feature = feature.to(device)\n\n        y_pred = torch.sigmoid(model(feature))\n\n        for pred in y_pred:\n            csv_writer.writerow([count, pred.item()])\n            count += 1","metadata":{"execution":{"iopub.status.busy":"2023-01-08T21:02:04.502825Z","iopub.execute_input":"2023-01-08T21:02:04.504799Z","iopub.status.idle":"2023-01-08T21:02:08.793401Z","shell.execute_reply.started":"2023-01-08T21:02:04.504765Z","shell.execute_reply":"2023-01-08T21:02:08.792207Z"},"trusted":true},"execution_count":11,"outputs":[]}]}