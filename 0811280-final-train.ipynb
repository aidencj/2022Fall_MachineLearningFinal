{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install feature_engine","metadata":{"execution":{"iopub.status.busy":"2023-01-08T20:53:10.841249Z","iopub.execute_input":"2023-01-08T20:53:10.841947Z","iopub.status.idle":"2023-01-08T20:53:20.964137Z","shell.execute_reply.started":"2023-01-08T20:53:10.841906Z","shell.execute_reply":"2023-01-08T20:53:20.962892Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Requirement already satisfied: feature_engine in /opt/conda/lib/python3.7/site-packages (1.4.0)\nRequirement already satisfied: scikit-learn>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from feature_engine) (1.0.2)\nRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.7/site-packages (from feature_engine) (1.21.6)\nRequirement already satisfied: statsmodels>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from feature_engine) (0.13.2)\nRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from feature_engine) (1.7.3)\nRequirement already satisfied: pandas>=1.0.3 in /opt/conda/lib/python3.7/site-packages (from feature_engine) (1.3.5)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.0.3->feature_engine) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.0.3->feature_engine) (2022.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=1.0.0->feature_engine) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=1.0.0->feature_engine) (1.0.1)\nRequirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.7/site-packages (from statsmodels>=0.11.1->feature_engine) (21.3)\nRequirement already satisfied: patsy>=0.5.2 in /opt/conda/lib/python3.7/site-packages (from statsmodels>=0.11.1->feature_engine) (0.5.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=21.3->statsmodels>=0.11.1->feature_engine) (3.0.9)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from patsy>=0.5.2->statsmodels>=0.11.1->feature_engine) (1.15.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom feature_engine.encoding import WoEEncoder\nfrom sklearn.linear_model import HuberRegressor\nfrom sklearn.impute import KNNImputer","metadata":{"execution":{"iopub.status.busy":"2023-01-08T20:53:20.966819Z","iopub.execute_input":"2023-01-08T20:53:20.967256Z","iopub.status.idle":"2023-01-08T20:53:20.974018Z","shell.execute_reply.started":"2023-01-08T20:53:20.967203Z","shell.execute_reply":"2023-01-08T20:53:20.972943Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"INPUT_PATH = '/kaggle/input/tabular-playground-series-aug-2022/'\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-01-08T20:53:20.975613Z","iopub.execute_input":"2023-01-08T20:53:20.976614Z","iopub.status.idle":"2023-01-08T20:53:20.988631Z","shell.execute_reply.started":"2023-01-08T20:53:20.976568Z","shell.execute_reply":"2023-01-08T20:53:20.987695Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Seed\nseed = 123\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\nnp.random.seed(seed)\ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2023-01-08T20:53:20.991339Z","iopub.execute_input":"2023-01-08T20:53:20.992108Z","iopub.status.idle":"2023-01-08T20:53:21.001027Z","shell.execute_reply.started":"2023-01-08T20:53:20.992072Z","shell.execute_reply":"2023-01-08T20:53:21.000033Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n        \n    def __getitem__(self, index):\n        return self.X[index], self.y[index]\n    \n    def __len__(self):\n        return self.X.shape[0]\n\n\nclass ValidationDataset(Dataset):\n    def __init__(self, X):\n        self.X = X\n        \n    def __getitem__(self, index):\n        return self.X[index]\n    \n    def __len__(self):\n        return self.X.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-01-08T20:53:21.002350Z","iopub.execute_input":"2023-01-08T20:53:21.003255Z","iopub.status.idle":"2023-01-08T20:53:21.011771Z","shell.execute_reply.started":"2023-01-08T20:53:21.003220Z","shell.execute_reply":"2023-01-08T20:53:21.010842Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(os.path.join(INPUT_PATH, 'train.csv'), index_col='id')\ndf_test = pd.read_csv(os.path.join(INPUT_PATH, 'test.csv'), index_col='id')\ntarget, groups = df_train['failure'], df_train['product_code']\ndf_train.drop('failure',axis=1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-01-08T20:53:21.013235Z","iopub.execute_input":"2023-01-08T20:53:21.013775Z","iopub.status.idle":"2023-01-08T20:53:21.159182Z","shell.execute_reply.started":"2023-01-08T20:53:21.013741Z","shell.execute_reply":"2023-01-08T20:53:21.158201Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def Preprocessing(df_train, df_test, target):\n    # Concatenate training and testing data\n    data = pd.concat([df_train, df_test])\n    \n    # Use dictionaries of dictionary to store the most correlated column according to the product code\n    most_correlated = {}\n    # We manually add data for 'measurement_17' (because it is the most important one among other measurement columns)\n    most_correlated['measurement_17'] = {\n        'A': ['measurement_5','measurement_6','measurement_8'],\n        'B': ['measurement_4','measurement_5','measurement_7'],\n        'C': ['measurement_5','measurement_7','measurement_8','measurement_9'],\n        'D': ['measurement_5','measurement_6','measurement_7','measurement_8'],\n        'E': ['measurement_4','measurement_5','measurement_6','measurement_8'],\n        'F': ['measurement_4','measurement_5','measurement_6','measurement_7'],\n        'G': ['measurement_4','measurement_6','measurement_8','measurement_9'],\n        'H': ['measurement_4','measurement_5','measurement_7','measurement_8','measurement_9'],\n        'I': ['measurement_3','measurement_7','measurement_8']\n    }\n    \n    # From measurement_3 to measurement_16, calculate the sum of the largest 3 correlation values\n    m_cols = [f'measurement_{i}' for i in range(18)]\n    corr_values = []\n    for i in range(3, 17):\n        cur_col = m_cols[i]\n        df_correlation = np.abs(data[m_cols].corr()[cur_col]).sort_values(ascending=False)\n        corr_values.append([cur_col, np.sum(df_correlation[1:4])])\n    \n    # Sorting\n    corr_values = np.array(corr_values)\n    corr_values = corr_values[np.argsort(corr_values[:, 1])[::-1]]\n    \n    # For the 10 most correlated measurement columns\n    # Find other 4 columns that are most correlated to it, and store to dict\n    product_codes = data.product_code.unique()\n    for i in range(10):\n        cur_col = corr_values[i][0]\n        cur_correlated = {}\n        for code in product_codes:\n            df_correlation = np.abs(data[data.product_code == code][m_cols].corr()[cur_col]).sort_values(ascending=False)\n            cur_correlated[code] = df_correlation[1:5].index.tolist()\n        most_correlated[cur_col] = cur_correlated\n    \n    # features that need imputation (measurement columns + loading)\n    features = m_cols + ['loading']\n    \n    # For columns that are highly correlated to other columns, impute with linear model (HuberRegressor)\n    # For all other columns, use KNN imputer\n    for code in product_codes:\n        for cur_col in list(most_correlated.keys()):\n            temp = data[data.product_code == code]\n            corr_cols = most_correlated[cur_col][code]\n            temp_train = temp[corr_cols+[cur_col]].dropna(how='any')\n            temp_test = temp[(temp[cur_col].isnull()) & (temp[corr_cols].isnull().sum(axis=1)==0)]\n            \n            linear_model = HuberRegressor(epsilon=1.9, max_iter=400)\n            linear_model.fit(temp_train[corr_cols], temp_train[cur_col])\n            pred = linear_model.predict(temp_test[corr_cols])\n            data.loc[(data.product_code == code)&(data[cur_col].isnull())&(data[corr_cols].isnull().sum(axis=1)==0), cur_col] = pred\n        \n        knn_model = KNNImputer(n_neighbors=3)\n        data.loc[data.product_code == code, features] = knn_model.fit_transform(data.loc[data.product_code == code, features])\n        \n    \n    # DataFrame of preprocessed data, we will have a total of 10 features\n    preprocessed_data = pd.DataFrame()\n    preprocessed_data['m3_missing'] = data['measurement_3'].isnull().astype(np.int32)\n    preprocessed_data['m5_missing'] = data['measurement_5'].isnull().astype(np.int32)\n    preprocessed_data['area'] = data['attribute_2'] * data['attribute_3']\n    preprocessed_data['measurement_avg'] = data[[f'measurement_{i}' for i in range(3, 17)]].mean(axis=1)\n    # Other features that will be used for model\n    useful_cols = ['loading', 'attribute_0', 'measurement_17', 'measurement_0', 'measurement_1', 'measurement_2']\n    preprocessed_data[useful_cols] = data[useful_cols]\n    \n    # Split training and testing data\n    df_train = preprocessed_data[:df_train.shape[0]]\n    df_test = preprocessed_data[df_train.shape[0]:]\n    \n    # Encode 'attribute_0' with WoEEncoder(Weight of Evidence)\n    woe_encoder = WoEEncoder(variables=['attribute_0'])\n    woe_encoder.fit(df_train, target)\n    df_train = woe_encoder.transform(df_train)\n    df_test = woe_encoder.transform(df_test)\n    \n    # Scale data\n    scaler = StandardScaler()\n    np_train = scaler.fit_transform(df_train)\n    np_test = scaler.transform(df_test)\n    \n    return np_train, np_test","metadata":{"execution":{"iopub.status.busy":"2023-01-08T20:53:21.160789Z","iopub.execute_input":"2023-01-08T20:53:21.161144Z","iopub.status.idle":"2023-01-08T20:53:21.180644Z","shell.execute_reply.started":"2023-01-08T20:53:21.161109Z","shell.execute_reply":"2023-01-08T20:53:21.179318Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"X_train, X_test = Preprocessing(df_train, df_test, target)","metadata":{"execution":{"iopub.status.busy":"2023-01-08T20:53:21.181906Z","iopub.execute_input":"2023-01-08T20:53:21.183840Z","iopub.status.idle":"2023-01-08T20:53:40.649883Z","shell.execute_reply.started":"2023-01-08T20:53:21.183804Z","shell.execute_reply":"2023-01-08T20:53:40.648913Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"train_ds = TrainDataset(torch.FloatTensor(X_train), torch.FloatTensor(target.to_numpy()))\ntrain_dl = DataLoader(train_ds, batch_size=10, num_workers=2, drop_last=True, shuffle=True)\ntest_ds = TrainDataset(torch.FloatTensor(X_train), torch.FloatTensor(target.to_numpy()))\ntest_dl = DataLoader(test_ds, batch_size=500, num_workers=2, drop_last=False, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-01-08T20:53:40.651529Z","iopub.execute_input":"2023-01-08T20:53:40.651896Z","iopub.status.idle":"2023-01-08T20:53:40.659629Z","shell.execute_reply.started":"2023-01-08T20:53:40.651860Z","shell.execute_reply":"2023-01-08T20:53:40.658459Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(10, 32) \n        self.layer_2 = nn.Linear(32, 32)\n        self.layer_3 = nn.Linear(32, 16)\n        self.layer_out = nn.Linear(16, 1)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.4)\n        self.batchnorm1 = nn.BatchNorm1d(32)\n        self.batchnorm2 = nn.BatchNorm1d(16)\n        \n        \n    def forward(self, x):\n        x = self.relu(self.layer_1(x))\n        x = self.batchnorm1(x)\n        x = self.dropout(x)\n        x = self.relu(self.layer_2(x))\n        x = self.batchnorm1(x)\n        x = self.dropout(x)\n        x = self.relu(self.layer_3(x))\n        x = self.batchnorm2(x)\n        x = self.layer_out(x)\n        return x;","metadata":{"execution":{"iopub.status.busy":"2023-01-08T20:53:40.663527Z","iopub.execute_input":"2023-01-08T20:53:40.663975Z","iopub.status.idle":"2023-01-08T20:53:40.673558Z","shell.execute_reply.started":"2023-01-08T20:53:40.663923Z","shell.execute_reply":"2023-01-08T20:53:40.672649Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"model = Model().to(device)\nloss_fn = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\nfor epoch in range(10):\n        print(f\"Epoch [{epoch}]\")\n        \n        model.train()\n        total_loss,count = 0,0\n        for feature, label in train_dl:\n            feature = feature.to(device)\n            label = label.to(device)\n\n            pred = model(feature)\n            \n            loss = loss_fn(pred.flatten(), label)\n            \n            total_loss += loss\n            count+=len(label)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        print(f'average train loss: {total_loss.item()/count}')\n        \n        model.eval()\n        total_loss,count = 0,0\n        for feature, label in test_dl:\n            feature = feature.to(device)\n            label = label.to(device)\n\n            pred = model(feature)\n            \n            loss = loss_fn(pred.flatten(), label)\n            \n            total_loss += loss\n            count+=len(label)\n        print(f'average test loss: {total_loss.item()/count}')\n            \ntorch.save(model.state_dict(), 'model_weights.pth')\n","metadata":{"execution":{"iopub.status.busy":"2023-01-08T20:53:40.674949Z","iopub.execute_input":"2023-01-08T20:53:40.675906Z","iopub.status.idle":"2023-01-08T20:55:56.412764Z","shell.execute_reply.started":"2023-01-08T20:53:40.675870Z","shell.execute_reply":"2023-01-08T20:55:56.411674Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Epoch [0]\naverage train loss: 0.05286800914300668\naverage test loss: 0.0010411656257304923\nEpoch [1]\naverage train loss: 0.051527767159860745\naverage test loss: 0.0010426452745833162\nEpoch [2]\naverage train loss: 0.0514116556428185\naverage test loss: 0.0010376075626900199\nEpoch [3]\naverage train loss: 0.05126478994166353\naverage test loss: 0.0010362371718206035\nEpoch [4]\naverage train loss: 0.05121289283025969\naverage test loss: 0.001037005208065333\nEpoch [5]\naverage train loss: 0.051171971480111496\naverage test loss: 0.0010386198860027757\nEpoch [6]\naverage train loss: 0.0511401560147723\naverage test loss: 0.0010387236164797955\nEpoch [7]\naverage train loss: 0.05111299916053114\naverage test loss: 0.0010376097880497345\nEpoch [8]\naverage train loss: 0.05107337799474266\naverage test loss: 0.0010355608060389565\nEpoch [9]\naverage train loss: 0.05107522030544317\naverage test loss: 0.00103737483313535\n","output_type":"stream"}]}]}