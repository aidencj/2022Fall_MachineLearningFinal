{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install feature_engine","metadata":{"execution":{"iopub.status.busy":"2023-01-09T06:28:31.984790Z","iopub.execute_input":"2023-01-09T06:28:31.985168Z","iopub.status.idle":"2023-01-09T06:28:41.817329Z","shell.execute_reply.started":"2023-01-09T06:28:31.985137Z","shell.execute_reply":"2023-01-09T06:28:41.816169Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Requirement already satisfied: feature_engine in /opt/conda/lib/python3.7/site-packages (1.4.0)\nRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from feature_engine) (1.7.3)\nRequirement already satisfied: pandas>=1.0.3 in /opt/conda/lib/python3.7/site-packages (from feature_engine) (1.3.5)\nRequirement already satisfied: statsmodels>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from feature_engine) (0.13.2)\nRequirement already satisfied: scikit-learn>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from feature_engine) (1.0.2)\nRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.7/site-packages (from feature_engine) (1.21.6)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.0.3->feature_engine) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.0.3->feature_engine) (2022.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=1.0.0->feature_engine) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=1.0.0->feature_engine) (1.0.1)\nRequirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.7/site-packages (from statsmodels>=0.11.1->feature_engine) (22.0)\nRequirement already satisfied: patsy>=0.5.2 in /opt/conda/lib/python3.7/site-packages (from statsmodels>=0.11.1->feature_engine) (0.5.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from patsy>=0.5.2->statsmodels>=0.11.1->feature_engine) (1.15.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nfrom feature_engine.encoding import WoEEncoder\nfrom sklearn.linear_model import HuberRegressor\nfrom sklearn.impute import KNNImputer\nimport csv","metadata":{"execution":{"iopub.status.busy":"2023-01-09T06:28:41.820970Z","iopub.execute_input":"2023-01-09T06:28:41.821310Z","iopub.status.idle":"2023-01-09T06:28:41.830521Z","shell.execute_reply.started":"2023-01-09T06:28:41.821278Z","shell.execute_reply":"2023-01-09T06:28:41.829617Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Seed\nseed = 123\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\nnp.random.seed(seed)\ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2023-01-09T06:28:41.832112Z","iopub.execute_input":"2023-01-09T06:28:41.832712Z","iopub.status.idle":"2023-01-09T06:28:41.840982Z","shell.execute_reply.started":"2023-01-09T06:28:41.832673Z","shell.execute_reply":"2023-01-09T06:28:41.840050Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"INPUT_PATH = '/kaggle/input/tabular-playground-series-aug-2022/'\nMODEL_WEIGHT_PATH = '/kaggle/input/model-weights'\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-01-09T06:28:41.842664Z","iopub.execute_input":"2023-01-09T06:28:41.843080Z","iopub.status.idle":"2023-01-09T06:28:41.850684Z","shell.execute_reply.started":"2023-01-09T06:28:41.843048Z","shell.execute_reply":"2023-01-09T06:28:41.849544Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class ValidationDataset(Dataset):\n    def __init__(self, X):\n        self.X = X\n        \n    def __getitem__(self, index):\n        return self.X[index]\n    \n    def __len__(self):\n        return self.X.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-01-09T06:28:41.855541Z","iopub.execute_input":"2023-01-09T06:28:41.855798Z","iopub.status.idle":"2023-01-09T06:28:41.864045Z","shell.execute_reply.started":"2023-01-09T06:28:41.855775Z","shell.execute_reply":"2023-01-09T06:28:41.863140Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(os.path.join(INPUT_PATH, 'train.csv'), index_col='id')\ndf_test = pd.read_csv(os.path.join(INPUT_PATH, 'test.csv'), index_col='id')\ntarget, groups = df_train['failure'], df_train['product_code']\ndf_train.drop('failure',axis=1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T06:28:41.865282Z","iopub.execute_input":"2023-01-09T06:28:41.865754Z","iopub.status.idle":"2023-01-09T06:28:42.009778Z","shell.execute_reply.started":"2023-01-09T06:28:41.865720Z","shell.execute_reply":"2023-01-09T06:28:42.008811Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def Preprocessing(df_train, df_test, target):\n    # Concatenate training and testing data\n    data = pd.concat([df_train, df_test])\n    \n    # Use dictionaries of dictionary to store the most correlated column according to the product code\n    most_correlated = {}\n    # We manually add data for 'measurement_17' (because it is the most important one among other measurement columns)\n    most_correlated['measurement_17'] = {\n        'A': ['measurement_5','measurement_6','measurement_8'],\n        'B': ['measurement_4','measurement_5','measurement_7'],\n        'C': ['measurement_5','measurement_7','measurement_8','measurement_9'],\n        'D': ['measurement_5','measurement_6','measurement_7','measurement_8'],\n        'E': ['measurement_4','measurement_5','measurement_6','measurement_8'],\n        'F': ['measurement_4','measurement_5','measurement_6','measurement_7'],\n        'G': ['measurement_4','measurement_6','measurement_8','measurement_9'],\n        'H': ['measurement_4','measurement_5','measurement_7','measurement_8','measurement_9'],\n        'I': ['measurement_3','measurement_7','measurement_8']\n    }\n    \n    # From measurement_3 to measurement_16, calculate the sum of the largest 3 correlation values\n    m_cols = [f'measurement_{i}' for i in range(18)]\n    corr_values = []\n    for i in range(3, 17):\n        cur_col = m_cols[i]\n        df_correlation = np.abs(data[m_cols].corr()[cur_col]).sort_values(ascending=False)\n        corr_values.append([cur_col, np.sum(df_correlation[1:4])])\n    \n    # Sorting\n    corr_values = np.array(corr_values)\n    corr_values = corr_values[np.argsort(corr_values[:, 1])[::-1]]\n    \n    # For the 10 most correlated measurement columns\n    # Find other 4 columns that are most correlated to it, and store to dict\n    product_codes = data.product_code.unique()\n    for i in range(10):\n        cur_col = corr_values[i][0]\n        cur_correlated = {}\n        for code in product_codes:\n            df_correlation = np.abs(data[data.product_code == code][m_cols].corr()[cur_col]).sort_values(ascending=False)\n            cur_correlated[code] = df_correlation[1:5].index.tolist()\n        most_correlated[cur_col] = cur_correlated\n    \n    # features that need imputation (measurement columns + loading)\n    features = m_cols + ['loading']\n    \n    # For columns that are highly correlated to other columns, impute with linear model (HuberRegressor)\n    # For all other columns, use KNN imputer\n    for code in product_codes:\n        for cur_col in list(most_correlated.keys()):\n            temp = data[data.product_code == code]\n            corr_cols = most_correlated[cur_col][code]\n            temp_train = temp[corr_cols+[cur_col]].dropna(how='any')\n            temp_test = temp[(temp[cur_col].isnull()) & (temp[corr_cols].isnull().sum(axis=1)==0)]\n            \n            linear_model = HuberRegressor(epsilon=1.9, max_iter=400)\n            linear_model.fit(temp_train[corr_cols], temp_train[cur_col])\n            pred = linear_model.predict(temp_test[corr_cols])\n            data.loc[(data.product_code == code)&(data[cur_col].isnull())&(data[corr_cols].isnull().sum(axis=1)==0), cur_col] = pred\n        \n        knn_model = KNNImputer(n_neighbors=3)\n        data.loc[data.product_code == code, features] = knn_model.fit_transform(data.loc[data.product_code == code, features])\n        \n    \n    # DataFrame of preprocessed data, we will have a total of 10 features\n    preprocessed_data = pd.DataFrame()\n    preprocessed_data['m3_missing'] = data['measurement_3'].isnull().astype(np.int32)\n    preprocessed_data['m5_missing'] = data['measurement_5'].isnull().astype(np.int32)\n    preprocessed_data['area'] = data['attribute_2'] * data['attribute_3']\n    preprocessed_data['measurement_avg'] = data[[f'measurement_{i}' for i in range(3, 17)]].mean(axis=1)\n    # Other features that will be used for model\n    useful_cols = ['loading', 'attribute_0', 'measurement_17', 'measurement_0', 'measurement_1', 'measurement_2']\n    preprocessed_data[useful_cols] = data[useful_cols]\n    \n    # Split training and testing data\n    df_train = preprocessed_data[:df_train.shape[0]]\n    df_test = preprocessed_data[df_train.shape[0]:]\n    \n    # Encode 'attribute_0' with WoEEncoder(Weight of Evidence)\n    woe_encoder = WoEEncoder(variables=['attribute_0'])\n    woe_encoder.fit(df_train, target)\n    df_train = woe_encoder.transform(df_train)\n    df_test = woe_encoder.transform(df_test)\n    \n    # Scale data\n    scaler = StandardScaler()\n    np_train = scaler.fit_transform(df_train)\n    np_test = scaler.transform(df_test)\n    \n    return np_train, np_test","metadata":{"execution":{"iopub.status.busy":"2023-01-09T06:28:42.011353Z","iopub.execute_input":"2023-01-09T06:28:42.011688Z","iopub.status.idle":"2023-01-09T06:28:42.032909Z","shell.execute_reply.started":"2023-01-09T06:28:42.011654Z","shell.execute_reply":"2023-01-09T06:28:42.031975Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"X_train, X_test = Preprocessing(df_train, df_test, target)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T06:28:42.035515Z","iopub.execute_input":"2023-01-09T06:28:42.035807Z","iopub.status.idle":"2023-01-09T06:28:59.934423Z","shell.execute_reply.started":"2023-01-09T06:28:42.035782Z","shell.execute_reply":"2023-01-09T06:28:59.933442Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(10, 32) \n        self.layer_2 = nn.Linear(32, 32)\n        self.layer_3 = nn.Linear(32, 16)\n        self.layer_out = nn.Linear(16, 1)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.4)\n        self.batchnorm1 = nn.BatchNorm1d(32)\n        self.batchnorm2 = nn.BatchNorm1d(16)\n        \n        \n    def forward(self, x):\n        x = self.relu(self.layer_1(x))\n        x = self.batchnorm1(x)\n        x = self.dropout(x)\n        x = self.relu(self.layer_2(x))\n        x = self.batchnorm1(x)\n        x = self.dropout(x)\n        x = self.relu(self.layer_3(x))\n        x = self.batchnorm2(x)\n        x = self.layer_out(x)\n        return x;","metadata":{"execution":{"iopub.status.busy":"2023-01-09T06:28:59.935786Z","iopub.execute_input":"2023-01-09T06:28:59.936239Z","iopub.status.idle":"2023-01-09T06:28:59.947257Z","shell.execute_reply.started":"2023-01-09T06:28:59.936202Z","shell.execute_reply":"2023-01-09T06:28:59.946322Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"val_ds = ValidationDataset(torch.FloatTensor(X_test))\nval_dl = DataLoader(val_ds, batch_size=500, num_workers=2, drop_last=False, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T06:28:59.948533Z","iopub.execute_input":"2023-01-09T06:28:59.949408Z","iopub.status.idle":"2023-01-09T06:28:59.956113Z","shell.execute_reply.started":"2023-01-09T06:28:59.949372Z","shell.execute_reply":"2023-01-09T06:28:59.955157Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"model = Model().to(device)\nmodel.load_state_dict(torch.load(os.path.join(MODEL_WEIGHT_PATH, 'model_weights.pth')))\n\ncount = 26570\nwith open('submission.csv', 'w', newline='') as csv_file:\n    csv_writer = csv.writer(csv_file)\n    csv_writer.writerow([\"id\", \"failure\"])\n    \n    model.eval()\n\n    for feature in val_dl:\n        feature = feature.to(device)\n\n        y_pred = torch.sigmoid(model(feature))\n\n        for pred in y_pred:\n            csv_writer.writerow([count, pred.item()])\n            count += 1","metadata":{"execution":{"iopub.status.busy":"2023-01-09T06:28:59.957266Z","iopub.execute_input":"2023-01-09T06:28:59.958247Z","iopub.status.idle":"2023-01-09T06:29:01.379572Z","shell.execute_reply.started":"2023-01-09T06:28:59.958211Z","shell.execute_reply":"2023-01-09T06:29:01.378385Z"},"trusted":true},"execution_count":22,"outputs":[]}]}