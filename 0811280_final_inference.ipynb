{"cells":[{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-01-09T06:28:31.985168Z","iopub.status.busy":"2023-01-09T06:28:31.984790Z","iopub.status.idle":"2023-01-09T06:28:41.817329Z","shell.execute_reply":"2023-01-09T06:28:41.816169Z","shell.execute_reply.started":"2023-01-09T06:28:31.985137Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: feature_engine in /opt/conda/lib/python3.7/site-packages (1.4.0)\n","Requirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from feature_engine) (1.7.3)\n","Requirement already satisfied: pandas>=1.0.3 in /opt/conda/lib/python3.7/site-packages (from feature_engine) (1.3.5)\n","Requirement already satisfied: statsmodels>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from feature_engine) (0.13.2)\n","Requirement already satisfied: scikit-learn>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from feature_engine) (1.0.2)\n","Requirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.7/site-packages (from feature_engine) (1.21.6)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.0.3->feature_engine) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.0.3->feature_engine) (2022.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=1.0.0->feature_engine) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=1.0.0->feature_engine) (1.0.1)\n","Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.7/site-packages (from statsmodels>=0.11.1->feature_engine) (22.0)\n","Requirement already satisfied: patsy>=0.5.2 in /opt/conda/lib/python3.7/site-packages (from statsmodels>=0.11.1->feature_engine) (0.5.2)\n","Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from patsy>=0.5.2->statsmodels>=0.11.1->feature_engine) (1.15.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["! pip install feature_engine"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-01-09T06:28:41.821310Z","iopub.status.busy":"2023-01-09T06:28:41.820970Z","iopub.status.idle":"2023-01-09T06:28:41.830521Z","shell.execute_reply":"2023-01-09T06:28:41.829617Z","shell.execute_reply.started":"2023-01-09T06:28:41.821278Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import StandardScaler\n","from feature_engine.encoding import WoEEncoder\n","from sklearn.linear_model import HuberRegressor\n","from sklearn.impute import KNNImputer\n","import csv"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-01-09T06:28:41.832712Z","iopub.status.busy":"2023-01-09T06:28:41.832112Z","iopub.status.idle":"2023-01-09T06:28:41.840982Z","shell.execute_reply":"2023-01-09T06:28:41.840050Z","shell.execute_reply.started":"2023-01-09T06:28:41.832673Z"},"trusted":true},"outputs":[],"source":["# To reproduce the same result, we manually set seed\n","seed = 123\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","np.random.seed(seed)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-01-09T06:28:41.843080Z","iopub.status.busy":"2023-01-09T06:28:41.842664Z","iopub.status.idle":"2023-01-09T06:28:41.850684Z","shell.execute_reply":"2023-01-09T06:28:41.849544Z","shell.execute_reply.started":"2023-01-09T06:28:41.843048Z"},"trusted":true},"outputs":[],"source":["INPUT_PATH = '/kaggle/input/tabular-playground-series-aug-2022/'\n","MODEL_WEIGHT_PATH = '/kaggle/input/model-weights'\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-01-09T06:28:41.855798Z","iopub.status.busy":"2023-01-09T06:28:41.855541Z","iopub.status.idle":"2023-01-09T06:28:41.864045Z","shell.execute_reply":"2023-01-09T06:28:41.863140Z","shell.execute_reply.started":"2023-01-09T06:28:41.855775Z"},"trusted":true},"outputs":[],"source":["class ValidationDataset(Dataset):\n","    # Dataset for validation data\n","    def __init__(self, X):\n","        self.X = X\n","        \n","    def __getitem__(self, index):\n","        return self.X[index]\n","    \n","    def __len__(self):\n","        return self.X.shape[0]"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-01-09T06:28:41.865754Z","iopub.status.busy":"2023-01-09T06:28:41.865282Z","iopub.status.idle":"2023-01-09T06:28:42.009778Z","shell.execute_reply":"2023-01-09T06:28:42.008811Z","shell.execute_reply.started":"2023-01-09T06:28:41.865720Z"},"trusted":true},"outputs":[],"source":["# Read the data\n","df_train = pd.read_csv(os.path.join(INPUT_PATH, 'train.csv'), index_col='id')\n","df_test = pd.read_csv(os.path.join(INPUT_PATH, 'test.csv'), index_col='id')\n","target = df_train['failure']\n","df_train.drop('failure',axis=1, inplace = True)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-01-09T06:28:42.011688Z","iopub.status.busy":"2023-01-09T06:28:42.011353Z","iopub.status.idle":"2023-01-09T06:28:42.032909Z","shell.execute_reply":"2023-01-09T06:28:42.031975Z","shell.execute_reply.started":"2023-01-09T06:28:42.011654Z"},"trusted":true},"outputs":[],"source":["def Preprocessing(df_train, df_test, target):\n","    # Preprocess the features and get the most useful 10 features.\n","    # Return: the scaled features in numpy.\n","\n","    # Concatenate training and testing data\n","    data = pd.concat([df_train, df_test])\n","    \n","    # Use dictionaries of dictionary to store the most correlated column according to the product code\n","    most_correlated = {}\n","    # We manually add data for 'measurement_17' (because it is the most important one among other measurement columns)\n","    most_correlated['measurement_17'] = {\n","        'A': ['measurement_5','measurement_6','measurement_8'],\n","        'B': ['measurement_4','measurement_5','measurement_7'],\n","        'C': ['measurement_5','measurement_7','measurement_8','measurement_9'],\n","        'D': ['measurement_5','measurement_6','measurement_7','measurement_8'],\n","        'E': ['measurement_4','measurement_5','measurement_6','measurement_8'],\n","        'F': ['measurement_4','measurement_5','measurement_6','measurement_7'],\n","        'G': ['measurement_4','measurement_6','measurement_8','measurement_9'],\n","        'H': ['measurement_4','measurement_5','measurement_7','measurement_8','measurement_9'],\n","        'I': ['measurement_3','measurement_7','measurement_8']\n","    }\n","    \n","    m_cols = [f'measurement_{i}' for i in range(18)]\n","    corr_values = []\n","    for i in range(3, 17):\n","        # From measurement_3 to measurement_16, calculate the sum of the largest 3 correlation values\n","        cur_col = m_cols[i]\n","        df_correlation = np.abs(data[m_cols].corr()[cur_col]).sort_values(ascending=False)\n","        corr_values.append([cur_col, np.sum(df_correlation[1:4])])\n","    \n","    # Sort according to the correlation values (in descending order)\n","    corr_values = np.array(corr_values)\n","    corr_values = corr_values[np.argsort(corr_values[:, 1])[::-1]]\n","    \n","    product_codes = data.product_code.unique()\n","    for i in range(10):\n","        # For the 10 most correlated measurement columns\n","        # Find other 4 columns that are most correlated to it, and store to dict\n","        cur_col = corr_values[i][0]\n","        cur_correlated = {}\n","        for code in product_codes:\n","            df_correlation = np.abs(data[data.product_code == code][m_cols].corr()[cur_col]).sort_values(ascending=False)\n","            cur_correlated[code] = df_correlation[1:5].index.tolist()\n","        most_correlated[cur_col] = cur_correlated\n","    \n","    # Features that need imputation (measurement columns + loading)\n","    features = m_cols + ['loading']\n","    \n","    for code in product_codes:\n","        # Impute features according to product code\n","        for cur_col in list(most_correlated.keys()):\n","            # For columns that are highly correlated to other columns, impute with linear model (HuberRegressor)\n","            temp = data[data.product_code == code]\n","            corr_cols = most_correlated[cur_col][code]\n","            temp_train = temp[corr_cols+[cur_col]].dropna(how='any')\n","            temp_test = temp[(temp[cur_col].isnull()) & (temp[corr_cols].isnull().sum(axis=1)==0)]\n","            \n","            linear_model = HuberRegressor(epsilon=1.9, max_iter=400)\n","            linear_model.fit(temp_train[corr_cols], temp_train[cur_col])\n","            pred = linear_model.predict(temp_test[corr_cols])\n","            data.loc[(data.product_code == code)&(data[cur_col].isnull())&(data[corr_cols].isnull().sum(axis=1)==0), cur_col] = pred\n","        \n","        # For all other columns (not highly correlated), use KNN imputer\n","        knn_model = KNNImputer(n_neighbors=3)\n","        data.loc[data.product_code == code, features] = knn_model.fit_transform(data.loc[data.product_code == code, features])\n","        \n","    \n","    # DataFrame of preprocessed data, we will have a total of 10 features\n","    preprocessed_data = pd.DataFrame()\n","    # New features\n","    preprocessed_data['m3_missing'] = data['measurement_3'].isnull().astype(np.int32)\n","    preprocessed_data['m5_missing'] = data['measurement_5'].isnull().astype(np.int32)\n","    preprocessed_data['area'] = data['attribute_2'] * data['attribute_3']\n","    preprocessed_data['measurement_avg'] = data[[f'measurement_{i}' for i in range(3, 17)]].mean(axis=1)\n","    # Old features\n","    useful_cols = ['loading', 'attribute_0', 'measurement_17', 'measurement_0', 'measurement_1', 'measurement_2']\n","    preprocessed_data[useful_cols] = data[useful_cols]\n","    \n","    # Split training and testing data\n","    df_train = preprocessed_data[:df_train.shape[0]]\n","    df_test = preprocessed_data[df_train.shape[0]:]\n","    \n","    # Encode 'attribute_0' with WoEEncoder(Weight of Evidence)\n","    woe_encoder = WoEEncoder(variables=['attribute_0'])\n","    woe_encoder.fit(df_train, target)\n","    df_train = woe_encoder.transform(df_train)\n","    df_test = woe_encoder.transform(df_test)\n","    \n","    # Scale data\n","    scaler = StandardScaler()\n","    np_train = scaler.fit_transform(df_train)\n","    np_test = scaler.transform(df_test)\n","    \n","    return np_train, np_test"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-01-09T06:28:42.035807Z","iopub.status.busy":"2023-01-09T06:28:42.035515Z","iopub.status.idle":"2023-01-09T06:28:59.934423Z","shell.execute_reply":"2023-01-09T06:28:59.933442Z","shell.execute_reply.started":"2023-01-09T06:28:42.035782Z"},"trusted":true},"outputs":[],"source":["X_train, X_test = Preprocessing(df_train, df_test, target)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-01-09T06:28:59.949408Z","iopub.status.busy":"2023-01-09T06:28:59.948533Z","iopub.status.idle":"2023-01-09T06:28:59.956113Z","shell.execute_reply":"2023-01-09T06:28:59.955157Z","shell.execute_reply.started":"2023-01-09T06:28:59.949372Z"},"trusted":true},"outputs":[],"source":["# Load the data\n","val_ds = ValidationDataset(torch.FloatTensor(X_test))\n","val_dl = DataLoader(val_ds, batch_size=500, num_workers=2, drop_last=False, shuffle=False)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-01-09T06:28:59.936239Z","iopub.status.busy":"2023-01-09T06:28:59.935786Z","iopub.status.idle":"2023-01-09T06:28:59.947257Z","shell.execute_reply":"2023-01-09T06:28:59.946322Z","shell.execute_reply.started":"2023-01-09T06:28:59.936202Z"},"trusted":true},"outputs":[],"source":["class Model(nn.Module):\n","    # NN Model for binary classification\n","    def __init__(self):\n","        super().__init__()\n","        self.layer_1 = nn.Linear(10, 32) \n","        self.layer_2 = nn.Linear(32, 32)\n","        self.layer_3 = nn.Linear(32, 16)\n","        self.layer_out = nn.Linear(16, 1)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(p=0.4)\n","        self.batchnorm1 = nn.BatchNorm1d(32)\n","        self.batchnorm2 = nn.BatchNorm1d(16)\n","        \n","        \n","    def forward(self, x):\n","        x = self.relu(self.layer_1(x))\n","        x = self.batchnorm1(x)\n","        x = self.dropout(x)\n","        x = self.relu(self.layer_2(x))\n","        x = self.batchnorm1(x)\n","        x = self.dropout(x)\n","        x = self.relu(self.layer_3(x))\n","        x = self.batchnorm2(x)\n","        x = self.layer_out(x)\n","        return x"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-01-09T06:28:59.958247Z","iopub.status.busy":"2023-01-09T06:28:59.957266Z","iopub.status.idle":"2023-01-09T06:29:01.379572Z","shell.execute_reply":"2023-01-09T06:29:01.378385Z","shell.execute_reply.started":"2023-01-09T06:28:59.958211Z"},"trusted":true},"outputs":[],"source":["model = Model().to(device)\n","# Load the model weight\n","model.load_state_dict(torch.load(os.path.join(MODEL_WEIGHT_PATH, 'model_weights.pth')))\n","\n","count = 26570\n","with open('submission.csv', 'w', newline='') as csv_file:\n","    csv_writer = csv.writer(csv_file)\n","    csv_writer.writerow([\"id\", \"failure\"])\n","    \n","    model.eval()\n","\n","    for feature in val_dl:\n","        feature = feature.to(device)\n","\n","        y_pred = torch.sigmoid(model(feature))\n","\n","        for pred in y_pred:\n","            csv_writer.writerow([count, pred.item()])\n","            count += 1"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":4}
